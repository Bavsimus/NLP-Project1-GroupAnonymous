# -*- coding: utf-8 -*-
"""DistilBERT_Transformer_FineTuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c26L75rJVC-nUxNHsVe12_Pw9fPXtpFq
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast
import warnings
warnings.filterwarnings('ignore')


try:
    df = pd.read_csv('spam_ham_dataset.csv')
except FileNotFoundError:
    print("Error: 'spam_ham_dataset.csv' not found. Please ensure the file is in the correct directory.")
    exit()


df['label_num'] = df['label'].map({'spam': 1, 'ham': 0})
X = df['text']
y = df['label_num']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Data split: Training={len(X_train)}, Test={len(X_test)}")



class SpamDataset(torch.utils.data.Dataset):
    """A custom Dataset class for handling tokenized text data in PyTorch."""
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.encodings['input_ids'])



print("\n--- 3. Tokenization and Encoding ---")
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')


train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True)
test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True)
print("Text data encoded successfully.")



train_dataset = SpamDataset(train_encodings, y_train.tolist())
test_dataset = SpamDataset(test_encodings, y_test.tolist())


train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16)
print("DataLoaders created successfully.")



print("\n--- 5. DistilBERT Fine-Tuning ---")
model_bert = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model_bert.to(device)

optimizer = AdamW(model_bert.parameters(), lr=5e-5)


model_bert.train()
EPOCHS = 3
for epoch in range(EPOCHS):
    for batch in train_loader:
        optimizer.zero_grad()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model_bert(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch + 1}/{EPOCHS} completed. Last Loss: {loss.item():.4f}")

print("DistilBERT training completed!")



print("\n--- 6. Evaluation Results ---")
model_bert.eval()

y_pred_bert = []
y_true_bert = []


for batch in test_loader:
    with torch.no_grad():
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model_bert(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)

        y_pred_bert.extend(predictions.cpu().tolist())
        y_true_bert.extend(labels.cpu().tolist())


accuracy_bert = accuracy_score(y_true_bert, y_pred_bert)
precision_bert = precision_score(y_true_bert, y_pred_bert)
cm_bert = confusion_matrix(y_true_bert, y_pred_bert)

print("--- DistilBERT Results (STATE-OF-THE-ART) ---")
print(f"Accuracy: {accuracy_bert:.4f}")
print(f"Precision: {precision_bert:.4f}")
print("Confusion Matrix:\n", cm_bert)